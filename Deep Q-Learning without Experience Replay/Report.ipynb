{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "- Name: Venkata Sameer Kumar Betana Bhotla\n",
    "- UIN: 728009992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 (20pts)** Implement DQN (with experience replay) using a Neural Network. Plot the reward per episode. This should give you an idea of convergence. You may have to use a  sliding average window  to get clean plots. Link your plot in the code block below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "This graph shows moving avergae rewards per episodes.  \n",
    "<img src=\"Fig3.jpeg\">\n",
    "\n",
    "This graph shows rewards per episodes while training. Orange line indicates the average rewards for 100 episodes.\n",
    "<img src=\"Fig1.png\">\n",
    "\n",
    "This graph shows rewards per episodes while testing. Orange line indicates the average rewards for entire testing.\n",
    "<img src=\"Fig2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (5pts)** Describe your Neural Network (number of hidden layers, hyparameters etc). \n",
    "\n",
    "**Answer:** \n",
    "- Nerual Network: \n",
    "    - Input size:  8 x 1 (size of the discrete state space)\n",
    "    - Output size: 4 x 1 (size of the discrete action space)\n",
    "    - Two hidden layers each of size 512 and 256 neurons.\n",
    "    - Activation function for both layers is ReLU and for output layer we have linear activation function. \n",
    "- Hyperparams: \n",
    "    - Learning rate $\\eta$: 0.001\n",
    "    - Epsilon (max) $\\epsilon$: 1.0\n",
    "    - Epsilon (min) $\\epsilon$: 0.1\n",
    "    - Epsilon (decay): 0.995 \n",
    "    - Gamma (discount factor) $\\gamma$: 0.99\n",
    "    - Episodes: 2000\n",
    "    - Replay memory buffer size: 500000\n",
    "    - Replay memory initial size: 50000 (i.e. we first get experience and then we start training)\n",
    "    - Batch size: 64\n",
    "    - Target network update: 10000 (i.e. we update the weights of traget network after 10000 epochs of total training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 (10pts)** You have to convince the world that you have landed on the moon (There are some who will not believe you), Link a video of your trained agent in the code block below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "This is the video of the Lunar Lander for episode where it got reward around ~250.\n",
    "<video width=\"600\" height=\"400\" controls>\n",
    "  <source src=\"LunarLander.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "This is the gif of the Lunar Lander for episode where it got reward around ~250.\n",
    "![](LunarLander.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4 (10 pts)** Implement DQN without experience replay. Plot the reward per episode. Link your plot in the code block below and briefly describe your observation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "This graph shows moving avergae rewards per episodes.  \n",
    "<img src=\"Fig4.jpeg\">\n",
    "\n",
    "This graph shows rewards per episodes while training. Orange line indicates the average rewards for 100 episodes.\n",
    "<img src=\"Fig5.png\">\n",
    "\n",
    "This graph shows rewards per episodes while testing. Orange line indicates the average rewards for entire testing.\n",
    "<img src=\"Fig6.png\">\n",
    "\n",
    "- When we don't have experience replay then it takes lot of time to learn a good model because, data points with valuable learning information are discraded after traning on them once. \n",
    "- Hence outcome of this is that we see lot of oscillations in rewards backforth while training and learning takes a lot of time to stabilize. Total no. of episodes to learn is also pretty huge. \n",
    "- Also, rewards don't go as high as in the previous case. \n",
    "\n",
    "This is the gif of the Lunar Lander for episode where it got reward around ~250.\n",
    "![](LunarLander1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 (5pts)** How would you train your agent if the action space is continuous?\n",
    "\n",
    "**Answer:** This problem is slightly different. Action space is continuous here. We can use algorithms like the DDPG algorithm to solve this problem. DDPG works quite well when we have continuous state and state space. In DDPG there are two networks called Actor and Critic. Actor-network output action value, given states to it. Critic network output the Q value (how good state-action pair is), given state and action(produces to by the actor-network) value pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6 (Bonus Question) (20pts)** Use a linear function approximator to train your agent. (*Hint: You might have to use kernels*)\n",
    "* Describe your function approximator\n",
    "* Plot the reward per episode \n",
    "* Link your video\n",
    "* Include your code with your submission \n",
    "\n",
    "**Answer:**\n",
    "- One way to solve this problem will be to use Radial Basis function (8 of them each corresponding to one component of the state vector) and pick centers randomly and right our Q-values as $q = \\sum w_{i}^{T}f(x_{i})$. \n",
    "- We can update the weights using simple stochastic gradient descent update. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
